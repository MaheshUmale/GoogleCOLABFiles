{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaheshUmale/GoogleCOLABFiles/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c169fd9d",
      "metadata": {
        "id": "c169fd9d"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community langchain-core\n",
        "\n",
        "\n",
        "# Install the Colab Terminal (optional, but helpful for running Ollama in the background)\n",
        "!pip install colab-xterm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "202624ad",
      "metadata": {
        "id": "202624ad"
      },
      "outputs": [],
      "source": [
        "# 1. Install Ollama (Linux version)\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# 2. Start the Ollama server in the background\n",
        "# We use 'nohup' and '&' to keep it running\n",
        "# You may see an error here about `tput: No such file or directory` - ignore it.\n",
        "!nohup ollama serve &\n",
        "print(\"Ollama server started in background.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e102b88",
      "metadata": {
        "id": "4e102b88"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "try:\n",
        "    # Ollama returns a simple '404 Not Found' response if running, which is correct for this endpoint.\n",
        "    response = requests.get(\"http://127.0.0.1:11434/\")\n",
        "    if response.status_code == 200:\n",
        "        print(\"âœ… Ollama API is active and listening on port 11434.\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ Ollama is running, but returned status code: {response.status_code}\")\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\"âŒ Connection Error: Ollama service is not reachable. Ensure it's running via `!nohup ollama serve &`.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb30f553",
      "metadata": {
        "id": "cb30f553"
      },
      "outputs": [],
      "source": [
        "!ollama pull deepseek-coder:6.7b-instruct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e9fb432",
      "metadata": {
        "id": "4e9fb432"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-classic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fb64722",
      "metadata": {
        "id": "3fb64722"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import sys\n",
        "from langchain_core.tools import tool\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "# Import the specific message classes required by the modern prompt\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "\n",
        "# Use the classic (stable) agent components\n",
        "from langchain_classic.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "# --- THE GUARANTEED SCRATCHPAD FIX ---\n",
        "def format_log_to_messages(intermediate_steps: list) -> list[BaseMessage]:\n",
        "    \"\"\"\n",
        "    Guaranteed custom function to convert the classic ReAct log (list of (Action, Observation) tuples)\n",
        "    into the list of BaseMessage objects required by ChatPromptTemplate (LangChain v1.x).\n",
        "\n",
        "    This directly resolves the final \"ValueError: variable agent_scratchpad should be a list of base messages\" error.\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    print(\"GOT MESSAGES ==============================================\")\n",
        "    # Each step is a tuple of (AgentAction, Observation)\n",
        "    for agent_action, observation in intermediate_steps:\n",
        "        # Convert the AgentAction (Thought + Action log) into an AIMessage\n",
        "        messages.append(AIMessage(content=agent_action.log))\n",
        "\n",
        "        # Convert the Observation into a HumanMessage\n",
        "        messages.append(HumanMessage(content=f\"Observation: {observation}\"))\n",
        "    return messages\n",
        "\n",
        "print(\"âœ… Custom scratchpad formatter defined successfully.\")\n",
        "\n",
        "\n",
        "\n",
        "## ðŸ› ï¸ Tool Definitions\n",
        "\n",
        "@tool\n",
        "def read_file_content(file_path: str) -> str:\n",
        "    \"\"\"Reads the full content of a file in the Colab working directory. Use this FIRST if the user specifies a file to read.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            content = f.read()\n",
        "        return f\"File '{file_path}' content:\\n\\n{content}\"\n",
        "    except FileNotFoundError:\n",
        "        return f\"ERROR: File not found at path: {file_path}. Did you upload it to Colab?\"\n",
        "    except Exception as e:\n",
        "        return f\"ERROR reading file: {e}\"\n",
        "\n",
        "@tool\n",
        "def execute_python_code(code: str) -> str:\n",
        "    \"\"\"Executes a block of Python code in a secure sandboxed environment. Use this to test a generated fix or verify existing code. Returns the output from STDOUT/STDERR.\"\"\"\n",
        "    try:\n",
        "        old_stdout = sys.stdout\n",
        "        redirected_output = sys.stdout = io.StringIO()\n",
        "        # Ensure 'exec' has access to the global scope if needed, though typically clean exec is safer\n",
        "        exec(code, {})\n",
        "        sys.stdout = old_stdout\n",
        "        output = redirected_output.getvalue()\n",
        "\n",
        "        return f\"Code Execution SUCCESS. Output:\\n{output.strip()}\" if output else \"Code Execution SUCCESS. No output was printed.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Code Execution FAILED. Error:\\n{type(e).__name__}: {e}\"\n",
        "\n",
        "deepcoder_tools = [read_file_content, execute_python_code]\n",
        "print(\"âœ… Agent Tools defined and ready.\")\n",
        "\n",
        "## ðŸ¤– LLM and Agent Configuration\n",
        "\n",
        "OLLAMA_BASE_URL = \"http://127.0.0.1:11434\"\n",
        "MODEL_NAME = \"deepseek-coder:6.7b-instruct\"\n",
        "\n",
        "llm = ChatOllama(model=MODEL_NAME, base_url=OLLAMA_BASE_URL, temperature=0.0)\n",
        "\n",
        "# 2. Define the System Prompt for ReAct\n",
        "system_prompt = (\n",
        "    \"You are DeepCoder, an expert Python debugging and development agent. \"\n",
        "    \"Your goal is to complete the user's task using the available tools.\"\n",
        "    \"\\n\\nAnswer the user's request using the available tools, which are:\\n{tools}\"\n",
        "    \"\\n\\nUse the following Thought-Action-Observation format. The only available actions are defined above.\"\n",
        "    \"\\n\\nThought: I need to analyze the user's request, possibly by reading a file, and then use the available tools to fix and test the code.\"\n",
        "    \"\\nAction: tool_name[tool_input]\"\n",
        "    \"\\nObservation: tool_output\"\n",
        "    \"\\n... (Continue Thought/Action/Observation until complete)\"\n",
        "    \"\\n\\nWhen the task is complete, use Final Answer: to provide the solution.\"\n",
        "    \"\\nFinal Answer: The complete, corrected Python code block enclosed in markdown fences (```python...```).\"\n",
        "    \"\\n\\nAvailable tools and their descriptions are listed below:\"\n",
        ")\n",
        "\n",
        "# 3. Create the Prompt Template\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "        # This placeholder is required by the ChatPromptTemplate\n",
        "        # (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "        # FIX IS HERE: Use MessagesPlaceholder\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ]\n",
        ").partial(tool_names=\", \".join([tool.name for tool in deepcoder_tools]))\n",
        "\n",
        "# 4. Create the Agent and Executor\n",
        "agent = create_react_agent(llm, deepcoder_tools, prompt)\n",
        "\n",
        "# Step B: Create the AgentExecutor with the formatter\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=deepcoder_tools,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        "    # Assign the custom, guaranteed function to handle the conversion\n",
        "    agent_scratchpad=format_log_to_messages,\n",
        ")\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=deepcoder_tools, verbose=True, handle_parsing_errors=True)\n",
        "print(f\"âœ… DeepCoder Agent Executor initialized successfully with stable ReAct agent.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a84371a4",
      "metadata": {
        "id": "a84371a4"
      },
      "outputs": [],
      "source": [
        "user_task = \"Fix the 'calculate_average' function in 'buggy_test.py' so it calculates the mean of two numbers correctly.  Read the file first, then confirm the fix by executing the code.\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"REQUEST: {user_task}\")\n",
        "print(\"=\"*70)\n",
        "# Wrap the string in a HumanMessage object and put it in a list\n",
        "formatted_input = [HumanMessage(content=user_task)]\n",
        "# response = agent_executor.invoke({\"input\": [HumanMessage(content=user_task)]})\n",
        "\n",
        "response = agent_executor.invoke({\"input\": user_query})\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"=== FINAL CORRECTED CODE (DeepCoder Agent) ===\")\n",
        "print(response['output'])\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a00b0af9",
      "metadata": {
        "id": "a00b0af9"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "MODEL_NAME = \"deepseek-coder:6.7b-instruct\"\n",
        "# Initialize the Ollama model\n",
        "llm = ChatOllama(model=MODEL_NAME) # Connects to the local server by default\n",
        "\n",
        "# Define a prompt template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Answer only in one sentence.\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Create a chain\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Invoke the chain\n",
        "response = chain.invoke({\"input\": \"Explain the concept of python array.\"})\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afff7d97",
      "metadata": {
        "id": "afff7d97"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import sys\n",
        "from langchain_core.tools import tool\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "# We need these classes to manually build the required list of messages\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "\n",
        "# Use the classic (stable) agent components\n",
        "from langchain_classic.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "# --- THE GUARANTEED SCRATCHPAD FIX (Final Code) ---\n",
        "def format_log_to_messages(intermediate_steps: list) -> list[BaseMessage]:\n",
        "    \"\"\"\n",
        "    Manually converts the classic ReAct log into the list of BaseMessage objects\n",
        "    required by the ChatPromptTemplate (resolves the ValueError).\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    # Each step is a tuple of (AgentAction, Observation)\n",
        "    for agent_action, observation in intermediate_steps:\n",
        "        # 1. Convert AgentAction (Thought + Action log) into an AIMessage\n",
        "        messages.append(AIMessage(content=agent_action.log))\n",
        "\n",
        "        # 2. Convert the Observation into a HumanMessage\n",
        "        messages.append(HumanMessage(content=f\"Observation: {observation}\"))\n",
        "    return messages\n",
        "\n",
        "print(\"âœ… Custom scratchpad formatter defined successfully.\")\n",
        "\n",
        "# --- 1. TOOL DEFINITIONS (Actions the Agent can take) ---\n",
        "\n",
        "@tool\n",
        "def read_file_content(file_path: str) -> str:\n",
        "    \"\"\"Reads the full content of a file in the Colab working directory. Use this FIRST if the user specifies a file to read.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            content = f.read()\n",
        "        return f\"File '{file_path}' content:\\n\\n{content}\"\n",
        "    except FileNotFoundError:\n",
        "        return f\"ERROR: File not found at path: {file_path}. Did you upload it to Colab?\"\n",
        "    except Exception as e:\n",
        "        return f\"ERROR reading file: {e}\"\n",
        "\n",
        "@tool\n",
        "def execute_python_code(code: str) -> str:\n",
        "    \"\"\"Executes a block of Python code in a secure sandboxed environment. Use this to test a generated fix or verify existing code. Returns the output from STDOUT/STDERR.\"\"\"\n",
        "    try:\n",
        "        old_stdout = sys.stdout\n",
        "        redirected_output = sys.stdout = io.StringIO()\n",
        "        exec(code, {})\n",
        "        sys.stdout = old_stdout\n",
        "        output = redirected_output.getvalue()\n",
        "\n",
        "        return f\"Code Execution SUCCESS. Output:\\n{output.strip()}\" if output else \"Code Execution SUCCESS. No output was printed.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Code Execution FAILED. Error:\\n{type(e).__name__}: {e}\"\n",
        "\n",
        "deepcoder_tools = [read_file_content, execute_python_code]\n",
        "print(\"âœ… Agent Tools defined and ready.\")\n",
        "\n",
        "# --- 2. LLM AND AGENT CONFIGURATION ---\n",
        "\n",
        "OLLAMA_BASE_URL = \"http://127.0.0.1:11434\"\n",
        "MODEL_NAME = \"deepseek-coder:6.7b-instruct\"\n",
        "\n",
        "llm = ChatOllama(model=MODEL_NAME, base_url=OLLAMA_BASE_URL, temperature=0.0)\n",
        "\n",
        "# 2. Define the System Prompt for ReAct\n",
        "system_prompt = (\n",
        "    \"You are DeepCoder, an expert Python debugging and development agent. \"\n",
        "    \"Your goal is to complete the user's task using the available tools.\"\n",
        "    \"\\n\\nAnswer the user's request using the available tools, which are:\\n{tools}\"\n",
        "    \"\\n\\nUse the following Thought-Action-Observation format. The only available actions are defined above.\"\n",
        "    \"\\n\\nThought: I need to analyze the user's request, possibly by reading a file, and then use the available tools to fix and test the code.\"\n",
        "    \"\\nAction: tool_name[tool_input]\"\n",
        "    \"\\nObservation: tool_output\"\n",
        "    \"\\n... (Continue Thought/Action/Observation until complete)\"\n",
        "    \"\\n\\nWhen the task is complete, use Final Answer: to provide the solution.\"\n",
        "    \"\\nFinal Answer: The complete, corrected Python code block enclosed in markdown fences (```python...```).\"\n",
        "    \"\\n\\nAvailable tools and their descriptions are listed below:\"\n",
        ")\n",
        "\n",
        "# 3. Create the Prompt Template\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "    ]\n",
        ").partial(tool_names=\", \".join([tool.name for tool in deepcoder_tools]))\n",
        "\n",
        "# 4. Create the Agent and Executor\n",
        "agent = create_react_agent(llm, deepcoder_tools, prompt)\n",
        "\n",
        "# Step B: Create the AgentExecutor with the formatter\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=deepcoder_tools,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        "    # This is the line that uses our guaranteed, custom function\n",
        "    agent_scratchpad=format_log_to_messages,\n",
        ")\n",
        "\n",
        "print(f\"âœ… DeepCoder Agent Executor initialized successfully with stable ReAct agent.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d54c4151",
      "metadata": {
        "id": "d54c4151"
      },
      "outputs": [],
      "source": [
        "# --- 2. Define the STICKY System Prompt for ReAct ---\n",
        "system_prompt = (\n",
        "    \"You are DeepCoder, an expert Python debugging and development agent. \"\n",
        "    \"Your goal is to complete the user's task using the available tools.\"\n",
        "    \"\\n\\n***STRICT FORMAT INSTRUCTIONS***:\"\n",
        "    \"\\n1. **DO NOT ADD ANY PREAMBLE OR CONVERSATIONAL TEXT.**\"\n",
        "    \"\\n2. Start your response **immediately** with a 'Thought:'.\"\n",
        "    \"\\n3. Use the Thought-Action-Observation format ONLY.\"\n",
        "    \"\\n4. Tools available: {tools}\"\n",
        "    \"\\n\\nUse the following exact format for actions:\"\n",
        "    \"\\nThought: I must read the file first.\"\n",
        "    \"\\nAction: tool_name[tool_input]\"\n",
        "    \"\\nObservation: tool_output\"\n",
        "    \"\\n... (Continue Thought/Action/Observation until complete)\"\n",
        "    \"\\n\\nWhen the task is complete, use Final Answer: to provide the solution.\"\n",
        "    \"\\nFinal Answer: ONLY the complete, corrected Python code block (```python...```).\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5fc0321",
      "metadata": {
        "id": "d5fc0321"
      },
      "outputs": [],
      "source": [
        "# --- Invocation Cell ---\n",
        "user_task = (\n",
        "    \"Fix the bug in 'buggy_test.py'. The function 'calculate_average' needs to correctly calculate the mean of the two numbers. \"\n",
        "    \"First, read the file, then verify the fixed code prints 15.0.\"\n",
        ")\n",
        "\n",
        "# Force the model to start with the correct Thought and Action sequence\n",
        "initial_scratchpad_hint = (\n",
        "    f\"Thought: I must first use the read_file_content tool to examine the file named buggy_test.py as requested by the user.\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"REQUEST: {user_task}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# The agent_executor.invoke expects 'input' and 'agent_scratchpad' (even though it's empty initially)\n",
        "response = agent_executor.invoke({\n",
        "    \"input\": user_task,\n",
        "    # Injecting the first thought here is often the key to making large models follow the format\n",
        "    \"agent_scratchpad\": initial_scratchpad_hint\n",
        "})\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"=== FINAL CORRECTED CODE (DeepCoder Agent) ===\")\n",
        "print(response['output'])\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f3bbb83",
      "metadata": {
        "id": "5f3bbb83"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import sys\n",
        "from langchain_core.tools import tool\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "\n",
        "from langchain_classic.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "# --- THE GUARANTEED SCRATCHPAD FIX (Function remains the same) ---\n",
        "def format_log_to_messages(intermediate_steps: list) -> str:\n",
        "    \"\"\"Fallback: Joins the classic agent log into a single string.\"\"\"\n",
        "    log = []\n",
        "    for action, observation in intermediate_steps:\n",
        "        log.append(action.log)\n",
        "        log.append(f\"Observation: {observation}\")\n",
        "    return \"\\n\".join(log)\n",
        "\n",
        "# --- 1. TOOL DEFINITIONS (Omitted for brevity, assume correct) ---\n",
        "@tool\n",
        "def read_file_content(file_path: str) -> str:\n",
        "    \"\"\"Reads the full content of a file in the Colab working directory. Use this FIRST if the user specifies a file to read.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            content = f.read()\n",
        "        return f\"File '{file_path}' content:\\n\\n{content}\"\n",
        "    except FileNotFoundError:\n",
        "        return f\"ERROR: File not found at path: {file_path}. Did you upload it to Colab?\"\n",
        "    except Exception as e:\n",
        "        return f\"ERROR reading file: {e}\"\n",
        "\n",
        "@tool\n",
        "def execute_python_code(code: str) -> str:\n",
        "    \"\"\"Executes a block of Python code in a secure sandboxed environment. Use this to test a generated fix or verify existing code. Returns the output from STDOUT/STDERR.\"\"\"\n",
        "    try:\n",
        "        old_stdout = sys.stdout\n",
        "        redirected_output = sys.stdout = io.StringIO()\n",
        "        exec(code, {})\n",
        "        sys.stdout = old_stdout\n",
        "        output = redirected_output.getvalue()\n",
        "\n",
        "        return f\"Code Execution SUCCESS. Output:\\n{output.strip()}\" if output else \"Code Execution SUCCESS. No output was printed.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Code Execution FAILED. Error:\\n{type(e).__name__}: {e}\"\n",
        "\n",
        "deepcoder_tools = [read_file_content, execute_python_code]\n",
        "\n",
        "# --- 2. LLM AND AGENT CONFIGURATION ---\n",
        "\n",
        "OLLAMA_BASE_URL = \"http://127.0.0.1:11434\"\n",
        "MODEL_NAME = \"deepseek-coder:6.7b-instruct\"\n",
        "\n",
        "llm = ChatOllama(model=MODEL_NAME, base_url=OLLAMA_BASE_URL, temperature=0.0)\n",
        "\n",
        "# 2. Define the System Prompt for ReAct (CRITICAL CHANGE HERE)\n",
        "template_string = (\n",
        "    \"You are DeepCoder, an expert Python debugging and development agent. \"\n",
        "    \"Your goal is to complete the user's task using the available tools.\"\n",
        "    \"\\n\\nAnswer the user's request using the available tools, which are:\\n{tools}\"\n",
        "    \"\\n\\nUse the following **EXACT** Thought-Action-Observation format. The only available actions are defined below.\"\n",
        "\n",
        "    \"\\n\\nThought: I must choose the correct tool and provide the necessary input.\"\n",
        "    \"\\nAction: [The name of the tool to use, e.g., read_file_content]\"\n",
        "    \"\\nAction Input: [The input string for the tool, e.g., 'buggy_test.py']\" # <--- THIS IS THE REQUIRED LINE\n",
        "\n",
        "    \"\\n\\nObservation: tool_output\"\n",
        "    \"\\n\\n{agent_scratchpad}\"\n",
        "    \"\\n\\nWhen the task is complete, use Final Answer: to provide the solution.\"\n",
        "    \"\\nFinal Answer: The complete, corrected Python code block enclosed in markdown fences (```python...```).\"\n",
        "    \"\\n\\n{input}\"\n",
        ")\n",
        "\n",
        "# 3. Create the Prompt Template (String-based)\n",
        "prompt = PromptTemplate.from_template(template_string).partial(\n",
        "    tool_names=\", \".join([tool.name for tool in deepcoder_tools]),\n",
        "    tools=str(deepcoder_tools)\n",
        ")\n",
        "\n",
        "# 4. Create the Agent and Executor\n",
        "agent = create_react_agent(llm, deepcoder_tools, prompt)\n",
        "\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=deepcoder_tools,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        "    agent_scratchpad=format_log_to_messages,\n",
        ")\n",
        "\n",
        "print(f\"âœ… DeepCoder Agent Executor configured. Please run the invocation cell now.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a986c865",
      "metadata": {
        "id": "a986c865"
      },
      "outputs": [],
      "source": [
        "# --- Invocation Cell with Final Path Fix ---\n",
        "\n",
        "# The full, absolute path is the safest way to ensure the agent finds the file.\n",
        "FILE_PATH_ABSOLUTE = \"/content/buggy_test.py\"\n",
        "\n",
        "user_task_fixed = (\n",
        "    \"Fix the bug in the file located at: \" + FILE_PATH_ABSOLUTE + \". The function 'calculate_average' needs to correctly calculate the mean of the two numbers. \"\n",
        "    \"Do not use quotes in the Action Input.\"\n",
        ")\n",
        "\n",
        "# Force the agent to start with the correct thought and the correct, absolute path.\n",
        "# We also explicitly tell it the Action Input should match the absolute path.\n",
        "initial_scratchpad_hint = (\n",
        "    f\"Thought: I must use the read_file_content tool to examine the file using the absolute path {FILE_PATH_ABSOLUTE}, without using quotes around the input.\"\n",
        "    f\"\\nAction: read_file_content\"\n",
        "    f\"\\nAction Input: {FILE_PATH_ABSOLUTE}\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"REQUEST: {user_task_fixed}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Invoke the agent executor\n",
        "response = agent_executor.invoke({\n",
        "    \"input\": user_task_fixed,\n",
        "    \"agent_scratchpad\": initial_scratchpad_hint\n",
        "})\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"=== FINAL CORRECTED CODE (DeepCoder Agent) ===\")\n",
        "print(response['output'])\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f259190",
      "metadata": {
        "id": "6f259190"
      },
      "outputs": [],
      "source": [
        "# --- RUN THIS CELL FIRST ---\n",
        "import os\n",
        "\n",
        "print(\"Contents of /content/ directory:\")\n",
        "# Use the shell command 'ls -l' to show the file names exactly\n",
        "# Note: The output might reveal a hidden character or extra quotes.\n",
        "!ls -l /content/\n",
        "\n",
        "# Check the exact, full path existence\n",
        "if os.path.exists(\"/content/buggy_test.py\"):\n",
        "    print(\"\\nâœ… OS CONFIRMS: /content/buggy_test.py EXISTS.\")\n",
        "else:\n",
        "    print(\"\\nâŒ OS DENIES: /content/buggy_test.py DOES NOT EXIST at that exact path.\")\n",
        "\n",
        "# If the OS denies, it is likely a capitalization or a hidden character.\n",
        "# We will use the absolute path in the next step based on the 'ls -l' output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8a4c7a5",
      "metadata": {
        "id": "e8a4c7a5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"Contents of /content/ directory:\")\n",
        "!ls -l /content/\n",
        "\n",
        "if os.path.exists(\"/content/buggy_test.py\"):\n",
        "    print(\"\\nâœ… SUCCESS: /content/buggy_test.py EXISTS. The agent will now work!\")\n",
        "else:\n",
        "    print(\"\\nâŒ FAILURE: File still missing. Please re-upload to the correct directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc5257dd",
      "metadata": {
        "id": "fc5257dd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}