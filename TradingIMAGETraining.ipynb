{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOH/RMNxNTH4DM/Xolmz1JP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaheshUmale/GoogleCOLABFiles/blob/main/TradingIMAGETraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rhzmO_dRbsL",
        "outputId": "67569ea9-c434-459c-8c10-f54cf759580c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define Your Permanent Paths ---\n",
        "# This makes your code clean and easy to manage\n",
        "DRIVE_PROJECT_PATH = '/content/drive/MyDrive/Colab_Trading_AI/'\n",
        "DATA_PATH = DRIVE_PROJECT_PATH + 'data/'\n",
        "MODEL_SAVE_PATH = DRIVE_PROJECT_PATH # Save models to the root project folder\n",
        "\n",
        "# --- Copy Data from Drive to Fast Session Storage (Run Once per Session) ---\n",
        "print(\"Copying zipped images from Drive to local session storage for speed...\")\n",
        "# The exclamation mark lets you run a shell command\n",
        "!cp '{DATA_PATH}trade_images_clean.zip' /content/\n",
        "\n",
        "print(\"Unzipping images...\")\n",
        "!unzip -q /content/trade_images_clean.zip -d /content/\n",
        "print(\"Unzipping complete. Images are now in /content/trade_images_clean/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv0JeDw-VjBT",
        "outputId": "702af56a-ede4-4a3a-9ee1-40137685c9f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying zipped images from Drive to local session storage for speed...\n",
            "Unzipping images...\n",
            "Unzipping complete. Images are now in /content/trade_images_clean/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '{DATA_PATH}hybrid_model_dataset.csv' /content/"
      ],
      "metadata": {
        "id": "_OsrYZ2qWGV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No major installs needed, TensorFlow is pre-installed on Colab\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, applications\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPmLG2h2V8L_",
        "outputId": "ef27727d-25a4-47fa-853f-406c39e57945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pAZ8n0AAg1dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "#\n",
        "#       COMPLETE SCRIPT FOR TRAINING A HYBRID TRADING AI MODEL\n",
        "#       Designed for Google Colab to prevent RAM crashes.\n",
        "#\n",
        "################################################################################\n",
        "\n",
        "# === SECTION 1: SETUP AND CONFIGURATION ===\n",
        "\n",
        "# --- 1.1 Connect to Google Drive for permanent storage ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 1.2 Import all necessary libraries ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import gc # Garbage Collector for memory management\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "\n",
        "# --- 1.3 Define all paths and training parameters ---\n",
        "# IMPORTANT: Make sure this path matches your folder structure in Google Drive\n",
        "DRIVE_PROJECT_PATH = '/content/drive/MyDrive/Colab_Trading_AI/'\n",
        "DATA_PATH = DRIVE_PROJECT_PATH + 'data/'\n",
        "MODEL_SAVE_PATH = DRIVE_PROJECT_PATH\n",
        "\n",
        "# Model and Data Parameters (Optimized from our discussion)\n",
        "MAX_SEQUENCE_LENGTH = 45  # To fit your longest description\n",
        "VOCAB_SIZE = 100          # To fit your ~72 unique words with a buffer\n",
        "IMG_SIZE = 224            # Required input size for MobileNetV2\n",
        "BATCH_SIZE = 32           # Number of samples per training step (can be 16 to save more RAM)\n",
        "EPOCHS = 15               # Number of times to train on the entire dataset\n",
        "\n",
        "# === SECTION 2: DATA PREPARATION ===\n",
        "\n",
        "# --- 2.1 Copy data from slow Drive to fast Colab session storage (runs once) ---\n",
        "print(\"Copying zipped images from Drive to local session storage for speed...\")\n",
        "!cp '{DATA_PATH}trade_images_clean.zip' /content/\n",
        "\n",
        "print(\"Unzipping images... (This may take a minute)\")\n",
        "!unzip -q /content/trade_images_clean.zip -d /content/\n",
        "print(\"Unzipping complete. Images are now in /content/trade_images_clean/\")\n",
        "\n",
        "# --- 2.2 Load the CSV from Google Drive ---\n",
        "print(\"Loading CSV from Google Drive...\")\n",
        "df = pd.read_csv(DATA_PATH + 'hybrid_model_dataset.csv')\n",
        "\n",
        "# --- 2.3 IMPORTANT: Adjust Image Paths ---\n",
        "# The paths in the CSV are relative. We prepend the local Colab path where we unzipped them.\n",
        "df['image_path'] = '/content/trade_images_clean/' + df['image_path'].astype(str)\n",
        "print(f\"Loaded {len(df)} records. Sample image path: {df['image_path'].iloc[0]}\")\n",
        "\n",
        "\n",
        "# === SECTION 3: TEXT PREPROCESSING ===\n",
        "\n",
        "print(\"Processing text data...\")\n",
        "# --- 3.1 Initialize and fit the tokenizer with our optimized vocabulary size ---\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(df['text_description'])\n",
        "\n",
        "# --- 3.2 Convert text to padded numerical sequences ---\n",
        "text_sequences = tokenizer.texts_to_sequences(df['text_description'])\n",
        "padded_text = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "# --- 3.3 Prepare labels and final image paths for splitting ---\n",
        "labels = df['label'].values\n",
        "image_paths = df['image_path'].values\n",
        "\n",
        "\n",
        "# === SECTION 4: EFFICIENT DATA PIPELINE (THE RAM CRASH SOLUTION) ===\n",
        "\n",
        "print(\"Splitting data and creating efficient tf.data pipelines...\")\n",
        "# --- 4.1 Split data into training (80%) and validation (20%) sets ---\n",
        "X_train_paths, X_val_paths, X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
        "    image_paths, padded_text, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# --- 4.2 Define the function that loads and preprocesses one image ---\n",
        "# This function is the core of the \"conveyor belt\" that prevents RAM overload.\n",
        "def load_and_preprocess_image(path):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
        "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image) # Preprocessing specific to MobileNetV2\n",
        "    return image\n",
        "\n",
        "# --- 4.3 Define the function that creates the complete dataset generator ---\n",
        "def create_dataset(image_paths, text_data, labels):\n",
        "    # Create datasets from the raw data (still very low RAM usage)\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    text_ds = tf.data.Dataset.from_tensor_slices(text_data.astype(np.int32))\n",
        "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
        "\n",
        "    # Zip the inputs and labels together in the required structure: ((input_1, input_2), label)\n",
        "    dataset = tf.data.Dataset.zip(((image_ds, text_ds), label_ds))\n",
        "\n",
        "    # Shuffle, batch, and prefetch for maximum performance\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# --- 4.4 Create the final training and validation datasets ---\n",
        "train_ds = create_dataset(X_train_paths, X_train_text, y_train)\n",
        "val_ds = create_dataset(X_val_paths, X_val_text, y_val)\n",
        "print(\"Data pipelines created successfully.\")\n",
        "\n",
        "\n",
        "# === SECTION 5: MEMORY MANAGEMENT ===\n",
        "\n",
        "print(\"Cleaning up large variables from RAM before training...\")\n",
        "# This is a crucial step to maximize available memory for the training process.\n",
        "del df, image_paths, padded_text, labels, text_sequences\n",
        "del X_train_paths, X_val_paths, X_train_text, X_val_text, y_train, y_val\n",
        "gc.collect() # Ask the garbage collector to reclaim the memory.\n",
        "print(\"Cleanup complete.\")\n",
        "\n",
        "\n",
        "# === SECTION 6: MODEL ARCHITECTURE ===\n",
        "\n",
        "print(\"Building the hybrid model...\")\n",
        "# --- 6.1 Vision Branch (\"The Eye\") using pre-trained MobileNetV2 ---\n",
        "base_vision_model = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "base_vision_model.trainable = False # Freeze the pre-trained weights\n",
        "\n",
        "image_input = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name='image_input')\n",
        "x = base_vision_model(image_input, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "vision_output = layers.Dense(128, activation='relu', name='vision_output')(x)\n",
        "\n",
        "# --- 6.2 Text Branch (\"The Ear\") ---\n",
        "text_input = layers.Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n",
        "y = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=64)(text_input)\n",
        "y = layers.LSTM(64)(y)\n",
        "text_output = layers.Dense(64, activation='relu', name='text_output')(y)\n",
        "\n",
        "# --- 6.3 Fusion and Classifier (\"The Brain\") ---\n",
        "combined = layers.Concatenate()([vision_output, text_output])\n",
        "z = layers.Dense(64, activation='relu')(combined)\n",
        "z = layers.Dropout(0.5)(z)\n",
        "z = layers.Dense(32, activation='relu')(z)\n",
        "final_output = layers.Dense(1, activation='sigmoid', name='final_output')(z)\n",
        "\n",
        "# --- 6.4 Create and Compile the final Hybrid Model ---\n",
        "hybrid_model = models.Model(inputs=[image_input, text_input], outputs=final_output)\n",
        "hybrid_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "hybrid_model.summary()\n",
        "\n",
        "\n",
        "# === SECTION 7: TRAINING THE MODEL ===\n",
        "\n",
        "print(\"\\nStarting model training...\")\n",
        "history = hybrid_model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_ds\n",
        ")\n",
        "print(\"\\nTraining complete.\")\n",
        "\n",
        "\n",
        "# === SECTION 8: SAVING THE FINAL ASSETS ===\n",
        "\n",
        "print(\"Saving trained model and tokenizer to Google Drive...\")\n",
        "# --- 8.1 Save the trained model ---\n",
        "hybrid_model.save(MODEL_SAVE_PATH + 'trading_hybrid_model_v1.h5')\n",
        "print(f\"Model saved to {MODEL_SAVE_PATH}trading_hybrid_model_v1.h5\")\n",
        "\n",
        "# --- 8.2 Save the tokenizer (CRITICAL for live prediction) ---\n",
        "with open(MODEL_SAVE_PATH + 'tokenizer_v1.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "print(f\"Tokenizer saved to {MODEL_SAVE_PATH}tokenizer_v1.pickle\")\n",
        "\n",
        "print(\"\\n\\n--- ALL STEPS COMPLETED SUCCESSFULLY ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MtiSnzpAggit",
        "outputId": "673943b6-f734-4233-8aa1-0053263d18d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "TensorFlow Version: 2.19.0\n",
            "Copying zipped images from Drive to local session storage for speed...\n",
            "Unzipping images... (This may take a minute)\n",
            "replace /content/trade_images_clean/NEGATIVE/360ONE/360ONE_20250730_124500.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "Unzipping complete. Images are now in /content/trade_images_clean/\n",
            "Loading CSV from Google Drive...\n",
            "Loaded 10532 records. Sample image path: /content/trade_images_clean/POSITIVE/360ONE/360ONE_20250729_120400.png\n",
            "Processing text data...\n",
            "Splitting data and creating efficient tf.data pipelines...\n",
            "Data pipelines created successfully.\n",
            "Cleaning up large variables from RAM before training...\n",
            "Cleanup complete.\n",
            "Building the hybrid model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mobilenetv2_1.00_2… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m,      │  \u001b[38;5;34m2,257,984\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m1280\u001b[0m)             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │      \u001b[38;5;34m6,400\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ mobilenetv2_1.00… │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m33,024\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ vision_output       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m163,968\u001b[0m │ global_average_p… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text_output (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m4,160\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ vision_output[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ text_output[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m12,352\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ final_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m33\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ mobilenetv2_1.00_2… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ mobilenetv2_1.00… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ vision_output       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │ global_average_p… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ vision_output[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ text_output[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ final_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,480,001\u001b[0m (9.46 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,480,001</span> (9.46 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m222,017\u001b[0m (867.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">222,017</span> (867.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting model training...\n",
            "Epoch 1/15\n",
            "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 111ms/step - accuracy: 0.8681 - loss: 0.4229 - val_accuracy: 0.8676 - val_loss: 0.4294\n",
            "Epoch 2/15\n",
            "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 104ms/step - accuracy: 0.8685 - loss: 0.4035 - val_accuracy: 0.8676 - val_loss: 0.3988\n",
            "Epoch 3/15\n",
            "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 107ms/step - accuracy: 0.8638 - loss: 0.4080 - val_accuracy: 0.8676 - val_loss: 0.4345\n",
            "Epoch 4/15\n",
            "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 102ms/step - accuracy: 0.8679 - loss: 0.3976 - val_accuracy: 0.8676 - val_loss: 0.4095\n",
            "Epoch 5/15\n",
            "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 103ms/step - accuracy: 0.8639 - loss: 0.3967 - val_accuracy: 0.8676 - val_loss: 0.3966\n",
            "Epoch 6/15\n",
            "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 103ms/step - accuracy: 0.8651 - loss: 0.3964 - val_accuracy: 0.8676 - val_loss: 0.3954\n",
            "Epoch 7/15\n",
            "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 102ms/step - accuracy: 0.8638 - loss: 0.3987 - val_accuracy: 0.8676 - val_loss: 0.3994\n",
            "Epoch 8/15\n",
            "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 94ms/step - accuracy: 0.8660 - loss: 0.3938 - val_accuracy: 0.8676 - val_loss: 0.4008\n",
            "Epoch 9/15\n",
            "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 95ms/step - accuracy: 0.8696 - loss: 0.3799 - val_accuracy: 0.8676 - val_loss: 0.4075\n",
            "Epoch 10/15\n",
            "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 92ms/step - accuracy: 0.8660 - loss: 0.3905 - val_accuracy: 0.8676 - val_loss: 0.4076\n",
            "Epoch 11/15\n",
            "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 92ms/step - accuracy: 0.8665 - loss: 0.3860 - val_accuracy: 0.8676 - val_loss: 0.3957\n",
            "Epoch 12/15\n",
            "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 107ms/step - accuracy: 0.8672 - loss: 0.3821 - val_accuracy: 0.8676 - val_loss: 0.3974\n",
            "Epoch 13/15\n",
            "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 91ms/step - accuracy: 0.8665 - loss: 0.3811 - val_accuracy: 0.8676 - val_loss: 0.4165\n",
            "Epoch 14/15\n",
            "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 108ms/step - accuracy: 0.8661 - loss: 0.3813 - val_accuracy: 0.8676 - val_loss: 0.4047\n",
            "Epoch 15/15\n",
            "\u001b[1m264/264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 89ms/step - accuracy: 0.8675 - loss: 0.3731 - val_accuracy: 0.8676 - val_loss: 0.4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training complete.\n",
            "Saving trained model and tokenizer to Google Drive...\n",
            "Model saved to /content/drive/MyDrive/Colab_Trading_AI/trading_hybrid_model_v1.h5\n",
            "Tokenizer saved to /content/drive/MyDrive/Colab_Trading_AI/tokenizer_v1.pickle\n",
            "\n",
            "\n",
            "--- ALL STEPS COMPLETED SUCCESSFULLY ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "djpHSnVXo9W4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}