{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"class17_Spam-Filter.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"0tAII5-XzHbx","colab_type":"text"},"source":["---\n","\n","# Data Mining:<br>Statistical Modeling and Learning from Data\n","\n","## Dr. Ciro Cattuto<br>Dr. Laetitia Gauvin<br>Dr. André Panisson\n","\n","### Exercises - Text Message Spam Filter\n","\n","---"]},{"cell_type":"code","metadata":{"id":"oWiDV8vJzHb2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"90317ea3-fffb-4fd6-ce8e-cc167cd9431d","executionInfo":{"status":"ok","timestamp":1581601676668,"user_tz":480,"elapsed":2487,"user":{"displayName":"Mahesh Umale","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBNJEMaoD40acveNoc8jBNHCip7My1WYuipnLEEdQ=s64","userId":"09190557123868067283"}}},"source":["%pylab inline"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Populating the interactive namespace from numpy and matplotlib\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sJUgAGvUzHb-","colab_type":"text"},"source":["# Building a Text Message Spam Filter with a Naive Bayes Classifier\n","\n","This tutorial explains how to classify text messages as spam / not spam using scikit-learn."]},{"cell_type":"markdown","metadata":{"id":"Z7B0v28OzHcB","colab_type":"text"},"source":["# Download Dataset\n","The SMS Spam Collection is open source and available at the UCI Machine Learning Repository.\n","The data files and documentation can be found here: http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n","\n","## Reading the Dataset into Memory"]},{"cell_type":"code","metadata":{"id":"lQ5EyQvAzHcD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":231},"outputId":"c5933011-04e2-4f51-a8d7-9c30c9ab4d5c","executionInfo":{"status":"error","timestamp":1581601678546,"user_tz":480,"elapsed":4352,"user":{"displayName":"Mahesh Umale","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBNJEMaoD40acveNoc8jBNHCip7My1WYuipnLEEdQ=s64","userId":"09190557123868067283"}}},"source":["messages = []\n","categories = []\n","for line in open(\"data/smsdata.txt\"):\n","    category, message = line.split('\\t')\n","    messages.append(message)\n","    categories.append(category)\n","\n","y = array([0 if item==\"ham\" else 1 for item in categories])\n"," \n","print()\n","print (\" %d Not Spam\" % (y==0).sum())\n","print (\"+ %d Spam\" % (y==1).sum())\n","print (\" ---------\")\n","print (\" %d Total\" % len(y))\n","print() \n","print (\"Proportion spam: %.2f/100\" % (100.*(y==1).sum() / float(len(y))))"],"execution_count":2,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-102aeb386857>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/smsdata.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/smsdata.txt'"]}]},{"cell_type":"markdown","metadata":{"id":"m7yKtGKDzHcJ","colab_type":"text"},"source":["# From Text Messages to Feature Vectors\n","We need to transform our text data into feature vectors, numerical representations which are suitable for performing statistical analysis. The most common way to do this is to apply a bag-of-words approach where the frequency of an occurrence of a word becomes a feature for our classifier.\n","\n","\n","## Term Frequency-Inverse Document Frequency\n","\n","We want to consider the relative importance of particular words, so we'll use term frequency–inverse document frequency as a weighting factor. This will control for the fact that some words are more \"spamy\" than others.\n","\n","## Mathematical details\n","\n","tf–idf is the product of two statistics, term frequency and inverse document\n","frequency. Various ways for determining the exact values of both statistics\n","exist. In the case of the '''term frequency''' tf(''t'',''d''), the simplest\n","choice is to use the ''raw frequency'' of a term in a document, i.e. the\n","number of times that term ''t'' occurs in document ''d''. If we denote the raw\n","frequency of ''t'' by f(''t'',''d''), then the simple tf scheme is\n","tf(''t'',''d'') = f(''t'',''d''). Other possibilities\n","include:\n","\n","  * boolean_data_type \"frequencies\": tf(''t'',''d'') = 1 if ''t'' occurs in ''d'' and 0 otherwise; \n","  * logarithmically scaled frequency: tf(''t'',''d'') = log (f(''t'',''d'') + 1); \n","  * augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the maximum raw frequency of any term in the document: :$\\mathrm{tf}(t,d) = 0.5 + \\frac{0.5 \\times \\mathrm{f}(t, d)}{\\max\\{\\mathrm{f}(w, d):w \\in d\\}}$\n","\n","The '''inverse document frequency''' is a measure of whether the term is\n","common or rare across all documents. It is obtained by dividing the total\n","number of documents by the number of documents containing the\n","term, and then taking the logarithm of that quotient.\n","\n",":$\\mathrm{idf}(t, D) = \\log \\frac{|D|}{|\\{d \\in D: t \\in d\\}|}$\n","\n","with\n","\n","  * $|D|$: cardinality of D, or the total number of documents in the corpus \n","  * $|\\{d \\in D: t \\in d\\}|$ : number of documents where the term $ t $ appears (i.e., $\\mathrm{tf}(t,d) eq 0$). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the formula to $1 + |\\{d \\in D: t \\in d\\}|$. \n","\n","Mathematically the base of the log function does not matter and constitutes a\n","constant multiplicative factor towards the overall result.\n","\n","Then tf–idf is calculated as\n","\n","$$\\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\times \\mathrm{idf}(t, D)$$"]},{"cell_type":"code","metadata":{"id":"AAw5_wqdzHcK","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","\n","pattern ='(?u)\\\\b[A-Za-z]{3,}'\n","\n","cv = CountVectorizer(stop_words=None, token_pattern=pattern,\n","                     ngram_range=(1, 3))\n","C = cv.fit_transform(messages)\n","\n","tfidf = TfidfTransformer(sublinear_tf=True)\n","#tfidf = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\n","                        \n","#calculate features using tf-idf and create a training set \n","X_train = tfidf.fit_transform(C)\n","print ()\n","print (\"X_train is a sparse matrix with shape: %s\" % str(X_train.shape))\n","print()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X_Aj25zRzHcR","colab_type":"text"},"source":["Here, we create a variable, tfidf, which is a vectorizer responsible for performing three important steps:\n","\n","- First, it will build a dictionary of features where keys are terms and values are indices of the term in the feature matrix (that's the fit part in fit_transform)\n","- Second, it will transform our documents into numerical feature vectors according to the frequency of words appearing in each text message. Since any one text message is short, each feature vector will be made up of mostly zeros, each of which indicates that a given word appeared zero times in that message.\n","- Lastly, it will compute the tf-idf weights for our term frequency matrix."]},{"cell_type":"markdown","metadata":{"id":"K7f-te5BzHcU","colab_type":"text"},"source":["# Naive Bayes\n","\n","Using Bayes' theorem, the conditional probability of observing a class $C_k$ given that we observed a set of features $\\mathbf{x}$ can be decomposed as\n","\n","$$p(C_k \\mid \\mathbf{x}) = \\frac{p(C_k) \\ p(\\mathbf{x} \\mid C_k)}{p(\\mathbf{x})}$$."]},{"cell_type":"markdown","metadata":{"id":"DM7HxiJ_zHcW","colab_type":"text"},"source":["### Bank example\n","\n","PC(Previous Client), CR(Criminal Record), Age, MP(Missed Payments), Res(Result of classification)"]},{"cell_type":"markdown","metadata":{"id":"dTkMPzJQzHcX","colab_type":"text"},"source":["<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n","    <colgroup>\n","        <col width=\"103\">\n","        <col width=\"103\">\n","        <col width=\"103\">\n","        <col width=\"103\">\n","        <col width=\"103\">\n","        <col width=\"103\">\n","    </colgroup>\n","\n","    <tbody>\n","        <tr>\n","            <td>\n","                <p>Id</p>\n","            </td>\n","\n","            <td>\n","                <p>PC</p>\n","            </td>\n","\n","            <td>\n","                <p>CR</p>\n","            </td>\n","\n","            <td>\n","                <p>Age</p>\n","            </td>\n","\n","            <td>\n","                <p>MP</p>\n","            </td>\n","\n","            <td>\n","                <p>Res</p>\n","            </td>\n","        </tr>\n","\n","        <tr>\n","            <td>\n","                <p>1</p>\n","            </td>\n","\n","            <td>\n","                <p>1</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>50</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>E</p>\n","            </td>\n","        </tr>\n","\n","        <tr>\n","            <td>\n","                <p>2</p>\n","            </td>\n","\n","            <td>\n","                <p>1</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>28</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>E</p>\n","            </td>\n","        </tr>\n","\n","        <tr>\n","            <td>\n","                <p>3</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>35</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>E</p>\n","            </td>\n","        </tr>\n","\n","        <tr>\n","            <td>\n","                <p>4</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>55</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>E</p>\n","            </td>\n","        </tr>\n","\n","        <tr>\n","            <td>\n","                <p>5</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>49</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>E</p>\n","            </td>\n","        </tr>\n","\n","        <tr>\n","            <td>\n","                <p>6</p>\n","            </td>\n","\n","            <td>\n","                <p>1</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>75</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>NE</p>\n","            </td>\n","        </tr>\n","\n","        <tr>\n","            <td>\n","                <p>7</p>\n","            </td>\n","\n","            <td>\n","                <p>1</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>38</p>\n","            </td>\n","\n","            <td>\n","                <p>10</p>\n","            </td>\n","\n","            <td>\n","                <p>NE</p>\n","            </td>\n","        </tr>\n","\n","        <tr>\n","            <td>\n","                <p>8</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>83</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>NE</p>\n","            </td>\n","        </tr>\n","\n","        <tr>\n","            <td>\n","                <p>9</p>\n","            </td>\n","\n","            <td>\n","                <p>1</p>\n","            </td>\n","\n","            <td>\n","                <p>1</p>\n","            </td>\n","\n","            <td>\n","                <p>44</p>\n","            </td>\n","\n","            <td>\n","                <p>5</p>\n","            </td>\n","\n","            <td>\n","                <p>NE</p>\n","            </td>\n","        </tr>\n","\n","        <tr>\n","            <td>\n","                <p>10</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>1</p>\n","            </td>\n","\n","            <td>\n","                <p>28</p>\n","            </td>\n","\n","            <td>\n","                <p>0</p>\n","            </td>\n","\n","            <td>\n","                <p>NE</p>\n","            </td>\n","        </tr>\n","    </tbody>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"a59AIevSzHcc","colab_type":"text"},"source":["Given that \n","\n","$$p(\\mathbf{x} \\mid C_k)  =  \\prod_{i=1}^{D} p(x_i \\mid C_k) $$\n","\n","then we have that\n","\n","$$p(C_k \\mid \\mathbf{x}) \\propto p(C_k) \\ \\prod_{i=1}^{D} p(x_i \\mid C_k) $$\n","\n","and then we choose the predicted class by:\n","\n","$$\\hat{y} = \\arg\\max_{C_k} p({C_k}) \\prod_{i=1}^{D} p(x_i \\mid {C_k})$$"]},{"cell_type":"markdown","metadata":{"id":"ognQb5aOzHcd","colab_type":"text"},"source":["For example, to predict the class for the following cases:\n","\n","<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" class=\"Table2\">\n","    <colgroup>\n","        <col width=\"103\">\n","        <col width=\"103\">\n","        <col width=\"103\">\n","        <col width=\"103\">\n","        <col width=\"103\">\n","        <col width=\"103\">\n","    </colgroup>\n","\n","    <tr class=\"Table21\">\n","        <td>\n","            <p class=\"P4\">Id</p>\n","        </td>\n","\n","        <td>\n","            <p class=\"P4\">PC</p>\n","        </td>\n","\n","        <td>\n","            <p class=\"P4\">CR</p>\n","        </td>\n","\n","        <td>\n","            <p class=\"P4\">Age</p>\n","        </td>\n","\n","        <td>\n","            <p class=\"P4\">MP</p>\n","        </td>\n","\n","        <td>\n","            <p class=\"P4\">Res</p>\n","        </td>\n","    </tr>\n","\n","    <tr class=\"Table21\">\n","        <td>\n","            <p class=\"P3\">11</p>\n","        </td>\n","\n","        <td>\n","            <p class=\"P3\">0</p>\n","        </td>\n","\n","        <td>\n","            <p class=\"P3\">0</p>\n","        </td>\n","\n","        <td>\n","            <p class=\"P3\">50</p>\n","        </td>\n","\n","        <td>\n","            <p class=\"P3\">0</p>\n","        </td>\n","\n","        <td>\n","            <p class=\"P4\">?</p>\n","        </td>\n","    </tr>\n","\n","    <tr class=\"Table21\">\n","        <td>\n","            <p class=\"P3\">12</p>\n","        </td>\n","\n","        <td>\n","            <p class=\"P3\">1</p>\n","        </td>\n","\n","        <td>\n","            <p class=\"P3\">0</p>\n","        </td>\n","\n","        <td>\n","            <p class=\"P3\">80</p>\n","        </td>\n","\n","        <td>\n","            <p class=\"P3\">0</p>\n","        </td>\n","\n","        <td>\n","            <p class=\"P4\">?</p>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"l2Nvi615zHci","colab_type":"text"},"source":["From:\n","\n","- $\\mathbf{C} \\in \\{\\mathbf{E}, \\mathbf{NE}\\}$, $p(\\mathbf{E}) = 1/2$, $p(\\mathbf{NE}) = 1/2$\n","\n","\n","- $\\mathbf{PC} \\in \\{0, 1\\}$, $p(0 \\mid \\mathbf{E}) = 3/5$, $p(1 \\mid \\mathbf{E}) = 2/5$, $p(0 \\mid \\mathbf{NE}) = 2/5$, $p(1 \\mid \\mathbf{NE}) = 3/5$\n","\n","\n","- $\\mathbf{CR} \\in \\{0, 1\\}$, $p(0 \\mid \\mathbf{E}) = 1$, $p(1 \\mid \\mathbf{E}) = 0$, $p(0 \\mid \\mathbf{NE}) = 3/5$, $p(1 \\mid \\mathbf{NE}) = 2/5$\n","\n","\n","- $\\mathbf{Age} \\in \\{\\lt 65, \\gt 65 \\}$, $p(\\lt 65 \\mid \\mathbf{E}) = 1$, $p(\\gt 65 \\mid \\mathbf{E}) = 0$, $p(\\lt 65 \\mid \\mathbf{NE}) = 3/5$, $p(\\gt 65 \\mid \\mathbf{NE}) = 2/5$\n","\n","\n","- $\\mathbf{MP} \\in \\{0, \\gt 0 \\}$, $p(0 \\mid \\mathbf{E}) = 1$, $p(\\gt 0 \\mid \\mathbf{E}) = 0$, $p(0 \\mid \\mathbf{NE}) = 3/5$, $p(\\gt 0 \\mid \\mathbf{NE}) = 2/5$\n","\n","For object 11, we have that:\n","\n","$p(\\mathbf{E} \\mid [ 0, 0, \\lt 65, 0] =  1/2 \\times 3/5 \\times 1 \\times 1 \\times 1 = 0.3$\n","\n","$p(\\mathbf{NE} \\mid [ 0, 0, \\lt 65, 0] = 1/2 \\times 2/5 \\times 3/5 \\times 3/5 \\times 3/5 = 0.04$"]},{"cell_type":"markdown","metadata":{"id":"nAA22tJwzHcl","colab_type":"text"},"source":["Naive Bayes can be modeled in several different ways. For example, to model a feature using a **normal** density function, we have that:\n","\n","$$p(x_i \\mid C_k)  =  \\frac{1}{{\\sigma_{ik} \\sqrt {2\\pi } }} e^{{ - \\left( {x - \\mu_{ik} } \\right)^2 } / {2\\sigma_{ik} ^2 }} $$\n","\n","and to model a feature using a **multinomial** density function, we have that the multinomial naive Bayes classifier becomes a linear classifier when expressed in log-space:\n","\n","\\begin{align}\n","\\log p(C_k|\\mathbf{x}) & \\varpropto \\log \\left( p(C_k) \\prod_{i=1}^n {p_{ki}}^{x_i} \\right) \\\\\n","                       & = \\log p(C_k) + \\sum_{i=1}^n x_i \\cdot \\log p_{ki}                 \\\\\n","                       & = b + \\mathbf{w}_k^\\top \\mathbf{x}\n","\\end{align}\n","\n","where $b = \\log p(C_k)$ and $w_{ki} = \\log p_{ki}$.\n"]},{"cell_type":"markdown","metadata":{"id":"t2LLIaISzHco","colab_type":"text"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"KSturyV5zHcp","colab_type":"text"},"source":["We'll be using SciKits' MultinomialNB, a Naive Bayes classifier effective for catching spam with the added benefits of scalability and low training time.\n","\n","MultinomialNB implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors $\\mathbf{w}_y = ( w_{y1},\\ldots, w_{yD})$ for each class $y$, where $D$ is the number of features (in text classification, the size of the vocabulary) and $w_{yi}$ is the probability $P(x_i \\mid y)$ of feature $i$ appearing in a sample belonging to class $y$.\n","The parameters $\\mathbf{w}_y$ are estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:\n","$w_{yi} = \\frac{ N_{yi} + \\alpha}{N_y + \\alpha n}$\n","where $N_{yi} = \\sum_{x \\in T} x_i$ is the number of times feature $i$ appears in a sample of class $y$ in the training set $T$, and $N_{y} = \\sum_{i=1}^{|T|} N_{yi}$ is the total count of all features for class $y$.\n","The smoothing priors $\\alpha \\ge 0$ accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting $\\alpha = 1$ is called *Laplace smoothing*, while $\\alpha < 1$ is called *Lidstone smoothing*.\n","\n","**Why Multinomial Naive Bayes**?   \n","Each row of the training set represents a document. A document is a list of $n$ words. If we consider that each word $w_i$ of the vocabulary appears in the vocabulary with probability $p_i$, then each document can be represented as a **multinomial distribution** with $n$ trials. The number of possible outcomes is the number of words in the vocabulary, and in each trial we choose a word from the vocabulary following the event probabilities $p_i$."]},{"cell_type":"code","metadata":{"id":"VcOl0wxlzHcr","colab_type":"code","colab":{}},"source":["#create a list of training labels. 1 is spam, 0 if ham\n","y_train = y\n"," \n","print (\"y_train is a list of categories: %s ...\" % str(y_train)[:70])\n","print (\"X_train has %d feature vectors\" % (X_train.shape[0]))\n","print (\"y_train has %d target classes\" %(len(y_train)))\n","print (\"dataset has %d rows\" %(len(messages)))\n","print\n"," \n","# create a Naive Bayes classifier\n","from sklearn.naive_bayes import MultinomialNB\n","clf = MultinomialNB()\n"," \n","clf.fit(X_train, y_train)\n","print (\"Trained MultinomialNB Classifier\")\n","print (\"Coefficients: %s ...\" % (str(clf.coef_)[:70]))\n","print (\"   Intercept: %s\" %(str(clf.intercept_)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P4nOF9K_zHcz","colab_type":"text"},"source":["We create a variable, y_train, which is simply a list of target classes which our classifier will be trained to identify. 1 indicates spam while 0 indicates ham, or non-spam.\n","\n","Then we fit the model by passing the X_train sparse matrix and y_train to our MultinomialNB classifier's fit function.\n","\n","#Classifying New Observations\n","\n","Now let's classify the test documents as spam or not spam and see how we did."]},{"cell_type":"code","metadata":{"id":"0LcJe4P8zHc1","colab_type":"code","colab":{}},"source":["test_messages = [\"Call MobilesDirect free on 08000938767 to update now! or2stoptxt\",\n","                 \"Call now for a free trial offer!\",\n","                 \"Hey Sam, want to get some pizza later?\",\n","                 \"idk my bff jill?\",\n","                 \"Free later for a beer? Call me now!\"]\n","                 \n","# extract features from raw text documents\n","C_test = cv.transform(test_messages)\n","X_test = tfidf.transform(C_test)\n"," \n","# MultinomialNB's predict classes directly\n","print (\"Classified: %s\" % clf.predict(X_test))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B6yqxfQCzHc7","colab_type":"text"},"source":["The predict function yields an array of True / False values (True for spam, False for not spam)."]},{"cell_type":"markdown","metadata":{"id":"Ye8E6o4wzHc9","colab_type":"text"},"source":["## Exercise:\n","\n","The classification results for the test documents are not very encouraging.   \n","Find the best parameter for the MultinomialNB model, and check the classification results for the test documents."]},{"cell_type":"code","metadata":{"id":"0d0QODqyzHc_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}